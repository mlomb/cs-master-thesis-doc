\section{Final words}
\subsection{Conclusions}



falta escribir la conclusion, pero los puntos importantes son:

\begin{itemize}
\item maybe implementing a custom engine was not a good idea. bugs and stuff
\item feature sets: havent changed much in a long time. thats why it is so hard find anything better
\item feature engineering slow with this kind of task, iteration times are extremely slow. This is why fishnet exists.
\item I have underestimated the impact in speed of the tradeoff? es mucho mas importante minimizar la cantidad de feature updates que otra cosa
\item ...
\end{itemize}

% lo ideal de un feature set son patrones que NO se den en simult√°neo? asi podes aprender mas usando menos neuronas

WIP:

The main takeaway from this work is that the \featureset{All} feature set is at a sweetspot that is hard to beat. It has all the information of the board, it is easy to implement, easy to compute which features are active and it has a low number of feature updates per move (1.58).

When building other feature sets, I have underestimated the cost of extra feature updates. The performance hit of making more updates is, in the feature sets I tested, greater than the performance gain of having more information available. In the end, NNUE networks are trying to reduce the number of dot products, which is what each feature update does.

[...]

\subsection{Future work}

Training NNUEs is a daunting task, and there are lots of variables that affect dramatically the performance of the networks. Many decisions were made in this work to reduce the scope of the project, so naturally many variables were left unexplored.

The following are some key points that could be explored in a future work: \\

% not much is known what makes a dataset good
\textbf{Dataset:} A great deal of effort is put into good training data. The training data I used was generated using very specific parameters: depth 9, 5000 nodes and selected opening books. It is known that higher depth data results in worse networks. It is believed that the reason is that data becomes too hard for the network to learn. Also datasets generated with different books also affect the performance of the network.  Generating new data is a very slow process so it is harder to experiment with, which means that not that much reasearch has been done in this area. \\
Filtering of the data (skipping checks, captures, etc.) also affects the performance dramatically, but it is a lot easier to work with since it can be done after the data has been generated. New filtering conditions can be tried. \\

\textbf{Alternative to PQR:} Instead of the loss function used to train PQR, the triplet loss function could be tried, where the anchor is the P position, the positive is the observed position (Q) and the negative is the random position (R). I don't expect this to improve the that much, but it is worth trying. \\

\textbf{Network architecture:} The architecture of NNUE-like networks has gone through multiple iterations since its inception. This work focused on the first and most basic iteration of it. Maybe it is worth exploring more complex architectures with a fixed feature set rather than a fixed architecture with a variable feature set.
Almost certainly try lower values of L2, which may bring better results. \\

\textbf{Feature sets:} There are many aspects of the game that could be tried as features. A good place to start looking for new features are existing handcrafted evaluations. I had many ideas for new feature sets but I had to discard them because the thesis was already too long.
