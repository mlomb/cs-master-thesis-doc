\section{Final words}
\subsection{Conclusions}

maybe implementing a custom engine was not a good idea. bugs and stuff

feature sets: havent changed much in a long time. thats why it is so hard find anything better

feature engineering slow with this kind of task, iteration times are extremely slow. This is why fishnet exists.

% lo ideal de un feature set son patrones que NO se den en simult√°neo? asi podes aprender mas usando menos neuronas

\subsection{Future work}

Training NNUEs is a daunting task, and there are lots of variables that affect dramatically the performance of the networks. Many decisions were made in this work to reduce the scope of the project, so naturally many variables were left unexplored.

The following are some key points that could be explored in a future work: \\

% not much is known what makes a dataset good
\textbf{Dataset:} A great deal of effort is put into good training data. The training data I used was generated using very specific parameters: depth 9, 5000 nodes and selected opening books. It is known that higher depth data results in worse networks. It is believed that the reason is that data becomes too hard for the network to learn. Also datasets generated with different books also affect the performance of the network.  Generating new data is a very slow process so it is harder to experiment with, which means that not that much reasearch has been done in this area. \\
Filtering of the data (skipping checks, captures, etc.) also affects the performance dramatically, but it is a lot easier to work with since it can be done after the data has been generated. New filtering conditions can be tried. \\

\textbf{Alternative to PQR:} Instead of the loss function used to train PQR, the triplet loss function could be tried, where the anchor is the P position, the positive is the observed position (Q) and the negative is the random position (R). I don't expect this to improve the that much, but it is worth trying. \\

\textbf{Network architecture:} The architecture of NNUE-like networks has gone through multiple iterations since its inception. This work focused on the first and most basic iteration of it. Maybe it is worth exploring more complex architectures with a fixed feature set rather than a fixed architecture with a variable feature set.
Almost certainly try lower values of L2, which may bring better results. \\

\textbf{Feature sets:} There are many aspects of the game that could be tried as features. A good place to start looking for new features are existing handcrafted evaluations. I had many ideas for new feature sets but I had to discard them because the thesis was already too long.
