\section{Experiments and results}

With the tools and methodology defined in the previous sections, we can now proceed to the experiments. Each run is determined by its configuration, that can be divided into the following categories:

\begin{itemize}
\item \textbf{Feature set}: Determinates the encoding of the position, and thus the number of inputs of the model. It conditions which patterns the network can learn.

\item \textbf{Network architecture}: The size of each layer in the network. The first layer is the feature transformer, and its size roughly determinates how many patterns the network can learn. The following two layers should be tiny due the NNUE architecture.

\item \textbf{Dataset}: blabla

\item \textbf{Training method}: Can choose to use either Stockfish evaluations or PQR triplets. This determinates the format of the samples as well as the loss function. Most experiments will train using Stockfish evaluations.

\item \textbf{Training hyperparameters}: The usual machine learning hyperparameters for training, such as batch size, learning rate and scheduler.
\end{itemize}

To assess the performance of a run or to compare a set of runs, the following indicators can be used:

[hablar de que hay runs particulares que miden otras cosas pero estas son generales]

\begin{itemize}
\item \textbf{Puzzle accuracy}:
\item \textbf{Relative ELO performance}:
\item \textbf{Inference performance (infs/s)}:
\item \textbf{Training time (not important, one time)}:
\end{itemize}


The experiments are all run in the same hardware: Intel 14900K CPU for dataset generation, batching and evaluation, and a single NVIDIA RTX 4090 24GB GPU for training.

\subsection{Baselines}

The objective is to find good baselines to compare the experiments that will follow.

P and KP
Network size?
Inference time?

a partir de cierto punto al loss le cuesta bajar pero las otras metricas siguen mejorando, asi que sigo entrenando

hacer side by side de Piece y KingPiece:

matriz con dos ejes: batch size y L1 size. con colores?

\begin{figure}[H]
\centering
\makebox[\textwidth]{\includegraphics[width=\textwidth]{./dynamic/baselines_comparison.pdf}}
\caption{ASDASDASD}
\label{fig:asdasdasd}
\end{figure}


% TODO: experiment with the number of layers in L2 and L3

\newpage
\subsection{Axis encoding} % relevance
\label{sec:axis_encoding}

\newcommand{\axisarrows}[1]{\parbox{0.7cm}{\includegraphics[height=0.7cm]{../assets/arrows/#1.pdf}}}

\textbf{Motivation.} Looking back at the networks generated by \featureset{Piece} in previous runs, the learned weigths of most neurons in the feature transformer layer (L1) are related with the movement pattern of the pieces. Let's take the example in Figure \ref{fig:rook_weights}, which depicts the \featureset{Square} part of the features where the role is \symrook\ Rook.

\begin{figure}[h]
\centering
\subfloat[\centering $\white$ White]{{\includegraphics[width=7cm]{../assets/results/piece_weights/white_rook_weights.png} }}%
\qquad
\subfloat[\centering $\black$ Black]{{\includegraphics[width=7cm]{../assets/results/piece_weights/black_rook_weights.png} }}%
\caption{Weights of a neuron in the L1 layer, which are connected to features in \featureset{Piece} where the role is $\rook$ Rook. The intensity represents the weight value, and the color represents the sign (although not relevant).}
\label{fig:rook_weights}
\end{figure}

This particular neuron learned to recognize the presence of a rook, affected by the pattern of another potential rook in the same file or rank. Doing so, it had to relate one feature for every potential square where a rook could be for that specific center location, which restrains the network from learning more complex patterns and it is harder to train, because you need more samples to account for all possible combinations.

What if we add a feature which describes \enquote{\textit{there is a $\white$ White $\rook$ Rook in the 4th rank}}? Certainly, this would make the network's job easier, as it would only need to learn the presence of rooks in the corresponding file or rank, instead of every square. This idea can be extrapolated to diagonals, to ease patterns with $\bishop$ Bishops and the $\queen$ Queen.

More examples of this behaviour can be found in Appendix \ref{appendix:axis_samples}, showcasing diagonal patterns and the $\knight$ Knight movements, although they do not move straight through axes. \\

\textbf{Experiment.} In this first experiment, I will explore different positional encodings for the pieces on the board, combining the available axes. The canonical \featureset{Piece} feature set encodes each piece's position using the square it is located. Note that this is the same thing as encoding the position for a piece $P$ as $\featureset{File}_{P} \times \featureset{Rank}_{P}$. So the position of each piece is determined using the vertical (across ranks) and horizonal (across files) axes. There are los of ways to index the board, but I will stick to the most common axes:

\begin{table}[H]
\centering
\begin{tabular}{cccc}
\axisarrows{H} & \axisarrows{V} & \axisarrows{D1} & \axisarrows{D2} \\
Horizontal & Vertical & Diagonal 1 & Diagonal 2 \\
(across files) & (across ranks) &  & 
\end{tabular}
\end{table}

These axes coincide with the movement pattern of the pieces, which make them a good candidate to use as indexing dimensions. In table \ref{tab:axis_encoding} I present the feature sets that I decided to explore. The feature sets are named according to the axes they combine.

\begin{table}[H]
\caption{Axis encoding feature sets}
\label{tab:axis_encoding}
\centering

\newcommand{\rolecolor}{$\times$ $\featureset{R}_{P} \times \featureset{C}_{P}$}

\begin{tabular}{cccccc}
\toprule
\bf Depiction & \bf Feature set & \multicolumn{2}{c}{\makecell{\bf Definition\\for every piece $P$ in the board}} & \bf \makecell{\# of\\features} \\
\toprule
\axisarrows{HV} & \featureset{HV (Piece)} & $\featureset{File}_{P} \times \featureset{Rank}_{P}$ & \rolecolor & 768 \\
\midrule
\axisarrows{H} $\oplus$ \axisarrows{V} & $\featureset{H} \oplus \featureset{V}$ (\featureset{Compact}) & $(\featureset{File}_{P} \oplus \featureset{Rank}_{P})$ & \rolecolor & 192 \\
\midrule
\axisarrows{HV} $\oplus$ \axisarrows{H} $\oplus$ \axisarrows{V} & $\featureset{HV} \oplus \featureset{H} \oplus \featureset{V}$ & \makecell{$(\featureset{File}_{P} \times \featureset{Rank}_{P}$ $\oplus$ \\ $\featureset{File}_{P} \oplus \featureset{Rank}_{P})$} & \rolecolor & 960 \\
\midrule
\axisarrows{D1D2} & \featureset{D1D2} &  $\featureset{Diag1}_{P} \times \featureset{Diag2}_{P}$ & \rolecolor & 2700 \\
\midrule
\axisarrows{D1} $\oplus$ \axisarrows{D2} & $\featureset{D1} \oplus \featureset{D2}$ & $(\featureset{Diag1}_{P} \oplus \featureset{Diag2}_{P})$ & \rolecolor & 360 \\
\midrule
\axisarrows{HV} $\oplus$ \axisarrows{D1D2} & $\featureset{HV} \oplus \featureset{D1D2}$ & \makecell{$(\featureset{File}_{P} \times \featureset{Rank}_{P}$ $\oplus$ \\ $\featureset{Diag1}_{P} \times \featureset{Diag2}_{P})$} & \rolecolor & 3468 \\
\midrule
\makecell{\axisarrows{H} $\oplus$ \axisarrows{V} $\oplus$ \\ \axisarrows{D1} $\oplus$ \axisarrows{D2}} & \makecell{$\featureset{H} \oplus \featureset{V} \oplus$ \\ $\featureset{D1} \oplus \featureset{D2}$} & \makecell{$(\featureset{File}_{P} \oplus \featureset{Rank}_{P}$ $\oplus$ \\ $\featureset{Diag1}_{P} \oplus \featureset{Diag2}_{P})$} & \rolecolor & 552 \\
\midrule
\makecell{\axisarrows{HV} $\oplus$ \axisarrows{H} $\oplus$ \\ \axisarrows{V} $\oplus$ \axisarrows{D1} $\oplus$ \axisarrows{D2}} & \makecell{\featureset{HV} $\oplus$ \\\featureset{H} $\oplus$ \featureset{V} $\oplus$ \\\featureset{D1} $\oplus$ \featureset{D2}} & \makecell{$(\featureset{File}_{P} \times \featureset{Rank}_{P} \oplus$ \\ $\featureset{File}_{P} \oplus \featureset{Rank}_{P}$ $\oplus$ \\ $\featureset{Diag1}_{P} \oplus \featureset{Diag2}_{P})$} & \rolecolor & 1320 \\
\midrule
\axisarrows{HD1} $\oplus$ \axisarrows{VD2} & $\featureset{HD1} \oplus \featureset{VD2}$ & \makecell{$(\featureset{File}_{P} \times \featureset{Diag1}_{P}$ $\oplus$ \\ $\featureset{Rank}_{P} \times \featureset{Diag2}_{P})$} & \rolecolor & 2880 \\
\midrule
\axisarrows{VD1} $\oplus$ \axisarrows{HD2} & $\featureset{VD1} \oplus \featureset{HD2}$ & \makecell{$(\featureset{Rank}_{P} \times \featureset{Diag1}_{P}$ $\oplus$ \\ $\featureset{File}_{P} \times \featureset{Diag2}_{P})$} & \rolecolor & 2880 \\
\bottomrule

\multicolumn{5}{c}{\footnotesize \textbf{Note:} $\featureset{R}_{P} \times \featureset{C}_{P}$ expands to $\featureset{Role}_{P} \times \featureset{Color}_{P}$}

\end{tabular}

\end{table}

I expect that the feature sets that are sums of single axes ($\featureset{H} \oplus \featureset{V}, \featureset{D1} \oplus \featureset{D2}, \featureset{H} \oplus \featureset{V} \oplus \featureset{D1} \oplus \featureset{D2}$) will perform worse overall, as to capture the exact position of pieces in the board, the network will have to learn to relate two features for every location. This information is already available when there is a product of two dimensions.

\vspace{2cm}

\textbf{Results.}

RESULTADOS LOCO, RESULTADOS

\subsection{Symmetry}

Medir el impacto de agregar simetría al fs. Red mas chica, inf mas rapida, mejor perf?

probar simetria, eventualmente probar con el mejor feature set de arriba, a ver si mejora poniendo a cada bloque individual simetria

\featureset{Half-Relative(H|V|HV)King-Piece}?

\subsection{Piece movement}

Intentar capturar los patrones que se ven en P, asi se pueden reconocer patrones mas complejos.

\featureset{Piece-Move}

Bad perf.

\subsection{Statistical features}

Define \featureset{k-Piece-Piece}

\featureset{King-Piece} is a subset of \featureset{Piece-Piece}.

Top P

Hacer un subset de \featureset{PP} (589824).

\begin{itemize}
\item Destilar?
\item Probar si es lo mismo quedarse con el TOP K de las mas comunes o con las que dice el performance.
\item Catboost? PCA?
\end{itemize}

\subsection{Human behavior}

PQR human behaviour. Medir estilo Maia. comparar? no va a ser tan bueno.




% -----------------
hablar del tradeoff de los feature sets, la primera capa, y demás

vertical and horizontal data, probar dataset sin info vertical u horizonal / ambos y ver que pasa
ver si agregar capas posteriores ayuda o no "layer layers small increase in perf"

measure updates per move average and refreshes average per FS



\subsection{Active neurons}

medir si hay feature sets que no usen neuronas, que esto disparo el uso de HalfTopK

average number of features enabled by feature set (cantidad y porcentaje)



