\section{Engine implementation}

Building chess engines is a very discussed topic in the history of chess and thus very well documented. The Chess Programming Wiki (CPW) \cite{cpw} is a well known source of information to reference, which I will base my engine on. I aim to build a single-threaded classic engine and only make use of the most prominent optimizations to keep it simple. The engine strength is not that relevant, as it is only a tool to measure the relative performance of board encodings. However, a competent one is required. \\

Classic chess engines are composed of two main components: \textbf{the search} and \textbf{the evaluation}. The search is the process of exploring the tree of possible moves, which is what this chapter is about. The evaluation determines how good the positions are for who plays. As I mentioned in the introduction, classic engines used to use hand-crafted evaluations based on human knowledge. In my case, I will replace it entirely with a neural network, explained in the following chapters.

\subsection{Minimax search}

A position $p$ in chess is the state of the board with any other relevant information, like castling rights, en passant and the 50-move clock. Given a position $p$, we can call $f(p)$ its evaluation, a number that provides an assessment of how good the position is, computed either by a hand-crafted function or a neural network.

One approach to approximate a good move given a reasonable function $f$ could be to evaluate all possible positions that can be reached with a single move and choose the one that leads to the highest evaluation for the player who made the move. This idea can be extended to consider actions taken by the other player, and so on, to a fixed depth. Formally, this is called the minimax search algorithm \cite{minimax-survey:1995}.

In a minimax tree there are two kinds of nodes: maximizing nodes and minimizing nodes. 

\begin{itemize}
\item \textcolor{ForestGreen}{$\triangle$} \textbf{Maximizing nodes} are the ones where the player to move is our player. These nodes want to put the player in the best possible position, so they choose the action that maximizes the evaluation. Note that the root node is a maximizing node.
\item \textcolor{red}{\raisebox{0.1em}{\rotatebox[origin = c]{180}{$\triangle$}}} \textbf{Minimizing nodes} are the ones where the player to move is the opponent. These nodes want to put the player in the worst possible position, so they choose the action that minimizes the evaluation.
\end{itemize}


\begin{figure}[H]
\centering
% https://tikz.dev/tikz-trees
\begin{tikzpicture}[level distance=18mm,level/.style={sibling distance=70mm/#1},line width=0.5mm,minimum size=1.5cm, inner sep=-10mm]
    % regular polygon, regular polygon sides=3
\tikzstyle{max node}=[regular polygon, regular polygon sides=3,draw=ForestGreen]
\tikzstyle{min node}=[regular polygon, regular polygon sides=3,shape border rotate=180,draw=red]
\node[max node]{-7}
child{
    node[min node]{-10} edge from parent[draw=black]
    child {
        node[max node]{10} edge from parent[draw=black]
        child {
            node[min node]{10} edge from parent[draw=black]
            child {
                node[max node]{10} edge from parent[draw=black]
            }
            child {
                node[max node]{$+\infty$} edge from parent[draw=black]
            }
        }
        child {
            node[min node]{5} edge from parent[draw=black]
            child {
                node[max node]{5} edge from parent[draw=black]
            }
        }
    }
    child {
        node[max node]{-10} edge from parent[draw=black]
        child {
            node[min node]{-10} edge from parent[draw=black]
            child {
                node[max node]{-10} edge from parent[draw=black]
            }
        }
    }
}
child {
    node[min node]{-7} edge from parent[draw=blue]
    child {
        node[max node]{5} edge from parent[draw=black]
        child {
            node[min node]{5} edge from parent[draw=black]
            child {
                node[max node]{7} edge from parent[draw=black]
            }
            child {
                node[max node]{5} edge from parent[draw=black]
            }
        }
        child {
            node[min node]{$-\infty$} edge from parent[draw=black]
            child {
                node[max node]{$-\infty$} edge from parent[draw=black]
            }
        }
    }
    child {
        node[max node]{-7} edge from parent[draw=black]
        child {
            node[min node]{-7} edge from parent[draw=black]
            child {
                node[max node]{-7} edge from parent[draw=black]
            }
            child {
                node[max node]{-5} edge from parent[draw=black]
            }
        }
    }
}
;
\end{tikzpicture}
\caption{A minimax tree of depth 4. The best move for the maximizing player is the one that leads to the highest evaluation, marked in \textcolor{blue}{blue}.
}
\end{figure}

The algorithm recursively explores the tree to a fixed depth, evaluating the positions at the leaves with $f$. The evaluation is then propagated up the tree, alternating between maximizing and minimizing nodes, until it reaches the root node. After computing the whole tree to a fixed depth, the \enquote{best} move is defined by the move from the root node that maximizes the recursively computed evaluation (maximizing node).

Usually we do not want to run the search to a fixed depth, but rather for a fixed amount of time. The algorithm itself runs to a fixed depth, so what we can do is to run the search in a loop, staring from depth 1 and increasing it by one each iteration until the time runs out. This way the \enquote{best} move found so far is always available. Note that we can not draw conclusions from any unfinished search, so the \enquote{best} move is the one found at the last iteration. This approach is called iterative deepening, and when combined with a transposition table (a cache for evaluations) is very effective, making following iterations faster. \\

My implementation uses a variation of the minimax algorithm called \textit{negamax}. Negamax is a simplification of minimax that takes advantage of the zero-sum property of chess, meaning that an evaluation for a player is equivalent to the negation of the evaluation in the opponent's point of view. Instead of having two kinds of nodes, all nodes are maximizing nodes and the evaluation is negated after the recursion. This simplifies the implementation.

\subsection{Quiescence search}

The search algorithm runs to a fixed depth, which causes a horizon effect. The horizon effect manifests when the search stops at a position where a negative event (such a capture) is inevitable but due the fixed depth, the search results in weaker moves in an effort to avoid the inevitable, prefering branches where the negative event (the capture) has not happened yet.

\begin{figure}[H]
\centering
% https://tikz.dev/tikz-trees
\begin{tikzpicture}[level distance=15mm,level/.style={sibling distance=22mm/#1},line width=0.25mm]
\tikzstyle{root node}=[circle,draw,inner sep=3.0]
\tikzstyle{bad node}=[circle,inner sep=3.0,fill=red]
\tikzstyle{good node}=[circle,inner sep=3.0,fill=ForestGreen]
\tikzstyle{unk node}=[circle,inner sep=3.0,fill=gray]
\tikzstyle{best node}=[circle,inner sep=3.0,fill=blue]
\node(0-0)[root node]{}
child{
    node(1-0)[good node]{} edge from parent[draw=ForestGreen]
    child{
        node(2-0)[bad node]{} edge from parent[draw=red] node[left]{PxQ}
    }
    child{
        node(2-1)[good node]{}
        child{node(3-0)[unk node]{} edge from parent[dashed,draw=gray] node[left]{PxQ}}
        edge from parent[draw=ForestGreen]
    }
}
child{
    node(1-1)[bad node]{} edge from parent[draw=red]
    child{
        node(2-2)[bad node]{}
    }
    child{node(2-3)[bad node]{}}
    edge from parent node[left]{PxQ}
}
child{
    node(1-2)[good node]{} edge from parent[draw=ForestGreen]
    child{node(2-4)[bad node]{} edge from parent[draw=red] node[left]{PxQ}}
    child{
        node(2-5)[good node]{}
        child{node(3-1)[unk node]{} edge from parent[dashed,draw=gray] node[left]{PxQ}}
    }
}
;

%\draw[dashed,rounded corners=15,draw=orange]($(0-0)+(0.0,0.5)$) -- ($(2-0)+(-3.0,-0.3)$) -- ($(2-5)+(3.0,-0.3)$) -- cycle;
%\node[text=orange] at($(0-0)+(3.0,-0.5)$){\makecell{search up to\\depth 2}};

% horizon line
\draw[dashed,draw=blue]($(2-0)+(-2.5,-0.45)$) -- ($(2-5)+(2.5,-0.45)$);
% horizon label
\node[text=blue] at($(2-0)+(-1.8,-0.75)$){\makecell{horizon}};

\end{tikzpicture}
\caption{Demonstration of the horizon effect (not a minimax tree, only showing opponent nodes), when the search stops at depth 2. The capture PxQ (\sympawn\ Pawn takes \symqueen\ Queen) is inevitable. In the \textcolor{red}{red branches}, the capture has already happened. In the \textcolor{ForestGreen}{green branches}, the capture has not happened yet. Evaluations at the leaves favour the \textcolor{ForestGreen}{green positions} because those have an extra \symqueen\ Queen. Since losing the piece is inevitable, \textcolor{ForestGreen}{green positions} may actually be weaker than \textcolor{red}{red positions}, but the search does not know that.}
\end{figure}

To fix this, instead of returning the evaluation of the position at the leaves, an additional smaller search is done which only considers captures. This way the search can continue until a \enquote{quiet} position is reached, where no captures are available.

Since most of the positions the network will be evaluating are quiet due the quiescence search, it is important to make sure that the training set reflect that. Later on, only positions that are quiet will be used to train the network.

\subsection{Optimizations}

Many optimizations were made to the engine to make it reach a decent depth in a reasonable time, which makes the engine stronger. There are no novel improvements, most are well-known techniques that have been used in engines for decades and can be found in the Chess Programming Wiki \cite{cpw}.

The most prominent optimizations implemented are:

\begin{itemize}
\item \textbf{Alpha-beta pruning}: a way to eliminate big portions of the search tree by using the branch-and-bound technique. It allows to prune branches that are guaranteed to be worse than the most promising move found so far. This means that it does not affect the result of the search, it only makes it faster.

Each node in the search tree has two values associated with it: $\alpha$ and $\beta$. $\alpha$ is the best value found so far that the maximizing player can guarantee up to that node. $\beta$ is the best value found so far that the minimizing player can guarantee up to that node. Note that $\alpha \le \beta$ and the maximizing player tries to \enquote{push} $\alpha$ up and the minimizing player tries to \enquote{pull} $\beta$ down.

When a node is visited, the algorithm checks if $\alpha \ge \beta$. If this is the case, the branch can be pruned because the minimizing player can guarantee a value of $\beta$, which is worse than the best value found so far.

\item \textbf{Move ordering}: the order in which the moves are visited can have a big impact on the effectiveness of the alpha-beta pruning. If the move ordering is optimal, the effective branching factor is reduced to its square root, which means that the search can go twice as deep for the same amount of computation \cite[section 5.3.1]{ai_modern_approach}. In the worst case, it is identical to minimax. There are a couple of ways to improve move ordering, the most important being:
\begin{itemize}
\item \textbf{MVV/LVA} the most valuable victim, least valuable attacker is a simple heuristic that orders the moves by the value of the captured piece minus the value of the attacking piece. This way the most valuable captures are evaluated first, which are more likely to cause a cutoff.
\end{itemize}

\item \textbf{Transposition table}: during search, a position may be visited many times with different sequences of moves. This is called a transposition. The transposition table is a large hash table storing information about positions that have already been visited. This way, if a position is visited again, the engine can use the stored information to avoid re-evaluating it.

Even if the depth of the stored evaluation is lower than the current depth (insufficient to draw conclusions at the current depth), it can still be used to improve the move ordering.

\end{itemize}

Many other optimizations were made: history moves, killer moves, late move reductions, null move prunning, and more. They can be found in the documented code.

%- principal variation (mainline)
%- null move prunning
%- late move reductions
%- history moves, killer moves

\subsection{Implementation details}

%- Rust
%- I/O with UCI protocol
%- move generation using a fast library
%- POSITION STACK
%- time control

The engine is implemented in the Rust programming language. It uses the standard UCI protocol \footnote{Universal Chess Interface specification can be found \href{https://www.shredderchess.com/chess-features/uci-universal-chess-interface.html}{here}.} to communicate via standard input/output.

The most performance critical part of the engine aside from the evaluation is move generation, that is, given a position, list all available moves and make them.
Fortunately there is a battle-tested library for it called \textit{shakmaty}. The library provides a copy-make interface instead of a make-unmake one, so I have to rely on a stack of positions when doing recursion.

Time control is hard-coded to use the increment plus 2\% of the remaining time per move. Experiments run at a fixed time per move, so this is used in the Lichess arena.

By default the engine uses 128 MB of memory for the transposition table. This value will be used throughout the experiments and in the arena.

